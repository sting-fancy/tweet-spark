{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e3fe51b6-d6d9-49dc-8065-5018d0f7c0fa"
   },
   "source": [
    "## 2 - Stream files & aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "1b42649a-ae84-421b-ad33-5a3e6ee39d05"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from IPython.display import display, clear_output\n",
    "import uuid\n",
    "from time import sleep\n",
    "\n",
    "def makeFolder(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory '% s' created\" % path)\n",
    "    except BaseException as e:\n",
    "        print(\"Error on \",path,\" : \",e)\n",
    "\n",
    "stream_folder = './tweets_folder'\n",
    "sink_folder = './tweet_sink'\n",
    "checkpoint_folder = './tweet_checkpoint'\n",
    "\n",
    "#stream_folder shoudl exist\n",
    "makeFolder(sink_folder)\n",
    "makeFolder(checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "7b76f799-f77d-4c51-b4bd-b53c1d04618e"
   },
   "outputs": [],
   "source": [
    "# spark.stop() # stop session\n",
    "# start spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# enable suffling\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "2ca0ab80-0344-4ed7-8668-df9f6ecebf01"
   },
   "outputs": [],
   "source": [
    "# Reading incoming files in file streaming folder\n",
    "# no header in csv\n",
    "# setting maxFilesPerTrigger to consume files one at a time\n",
    "streamingDF = spark.readStream.format('csv')\\\n",
    "  .option(\"maxFilesPerTrigger\", 1)\\\n",
    "  .schema(schema='id long,text string')\\\n",
    "  .load(stream_folder)\n",
    "\n",
    "streamingDF.printSchema()\n",
    "print(\"Streaming DataFrame : \" + str(streamingDF.isStreaming))\n",
    "\n",
    "# FYI option to remove completed files\n",
    "# .option(\"cleanSource\",\"delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "f32c38d7-6a6b-49a5-b45f-58ea677d50d3"
   },
   "outputs": [],
   "source": [
    "# aggregation query for id\n",
    "streamingIDCountsDF = (\n",
    "  streamingDF\n",
    "    .groupBy(\n",
    "      streamingDF.id\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(\"Streaming streamingIDCountsDF : \" + str(streamingIDCountsDF.isStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "22eb4db4-f5fd-4854-956f-a00e3c62ee7b"
   },
   "outputs": [],
   "source": [
    "q_uuid = uuid.uuid4().hex\n",
    "\n",
    "# display real-time query\n",
    "query1 = (\n",
    "  streamingIDCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(q_uuid) \n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=False)\n",
    "    display(query1.status)\n",
    "    display(spark.sql('SELECT * FROM '+ q_uuid + ' ORDER BY 1').show())\n",
    "    sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "d864df32-b831-49c5-a254-0ad35c71ca1b"
   },
   "source": [
    "## check RT : re-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation query for id\n",
    "streaming_char = (\n",
    "  streamingDF\n",
    "    .groupBy(\n",
    "      streamingDF[\"text\"][0:2]\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(\"Streaming streaming_char : \" + str(streaming_char.isStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_uuid = uuid.uuid4().hex\n",
    "\n",
    "# display real-time query\n",
    "query2 = (\n",
    "  streaming_char\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(q_uuid) \n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=False)\n",
    "    display(query2.status)\n",
    "    display(spark.sql('SELECT * FROM '+ q_uuid + ' ORDER BY 2 DESC').show())\n",
    "    sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store RT - events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation query for id\n",
    "streaming_store = (\n",
    "  streamingDF\n",
    "    .withColumn(\"current_timestamp\", current_timestamp() )\\\n",
    "    .withColumn(\"contains_at\",streamingDF[\"text\"].rlike(\"@\"))\\\n",
    "    .withWatermark(\"current_timestamp\", \"1 minutes\") \\\n",
    "    .groupBy(\"contains_at\",\"current_timestamp\")\n",
    "    .count()\n",
    ")\n",
    "print(\"Streaming streaming_store : \" + str(streaming_store.isStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# enable suffling\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "# https://docs.databricks.com/structured-streaming/examples.html#foreachbatch-sqldw-example\n",
    "class SendToDynamoDB_ForeachWriter:\n",
    "\n",
    "  def open(self, partition_id, epoch_id): \n",
    "    self.file = open(filepath_each, 'a')\n",
    "    # header\n",
    "    self.file.write( str( ['current_timestamp'])  + \",\" + str( ['contains_at']) + \",\" +  str( ['count']) + \"\\n\" )\n",
    "    return True\n",
    "\n",
    "  def process(self, row): \n",
    "    # write row to file as it is processed \n",
    "    self.file.write( str(row['current_timestamp'])  + \",\" + str(row['contains_at']) + \",\" +  str(row['count']) + \"\\n\" )\n",
    "\n",
    "  def close(self, err):\n",
    "    self.file.close()\n",
    "    if err:\n",
    "      raise err\n",
    "\n",
    "\n",
    "# file to write to\n",
    "filepath_each = os.path.join(sink_folder, \"_store_for_each\"+ \".csv\")\n",
    "\n",
    "# initialise file\n",
    "fe = open(filepath_each, \"w\")\n",
    "fe.close()\n",
    "\n",
    "query_file = (\n",
    "  streaming_store\n",
    "    .toDF( \"current_timestamp\",\"contains_at\",\"count\" )\n",
    "    .writeStream\n",
    "    .foreach(SendToDynamoDB_ForeachWriter())\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "data_available = True\n",
    "\n",
    "while data_available:\n",
    "    clear_output(wait=False)\n",
    "    display(query_file.status)\n",
    "    if 'Initializing sources' in query_file.status[\"message\"] or 'Getting offsets' in query_file.status[\"message\"]:\n",
    "        pass\n",
    "    else:\n",
    "        data_available = query_file.status[\"isDataAvailable\"]\n",
    "    sleep(1)\n",
    "\n",
    "print('file ready')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
